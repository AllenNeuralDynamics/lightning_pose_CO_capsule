{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXyo-JJDpEJV"
   },
   "source": [
    "# ⚡ Generate a lightning-pose (LP) dataset and train a LP model ⚡\n",
    "This notebook shows how to convert the DeepLabCut (DLC) project to Lightning-Pose (LP) format and train LP model step by step. \n",
    "\n",
    "Here, we take Han's bottom-view DLC project and VBN project for example.\n",
    "<!-- * [Environment setup](#Environment-setup) -->\n",
    "* [Data preparation](#Data-preparation)\n",
    "<!-- * [Monitor optimization in real time (via TensorBoard UI)](#Monitor-training) -->\n",
    "* [Training](#Training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Materials for Lightning Pose:</b>\n",
    "    \n",
    "- [Paper](https://www.biorxiv.org/content/10.1101/2023.04.28.538703v1) shows a detailed mathematical description of the LP algorithm.\n",
    "\n",
    "- [GitHub](https://github.com/danbider/lightning-pose) and [Documentation](https://lightning-pose.readthedocs.io/en/latest/index.html) show how to implement LP.\n",
    "\n",
    "- Reference for this notebook at [here](https://github.com/danbider/lightning-pose/blob/7da5b5e701cb315ffd6d3ac8847191ee6715c46e/scripts/litpose_training_demo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Make sure to attach the data asset:</b>\n",
    "\n",
    "To do so, go to data/:\n",
    "* click the \"Manage Data Assets\" button \n",
    "* for Han's data: search \"han_video_s3\"\n",
    "* for VBN data: search \"vbn_dlc_all_4\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VXYyD0YFZfpE"
   },
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import os\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from lightning_pose.utils import pretty_print_cfg\n",
    "from lightning_pose.utils.io import (\n",
    "    return_absolute_data_paths,\n",
    ")\n",
    "from lightning_pose.utils.scripts import (\n",
    "    get_data_module,\n",
    "    get_dataset,\n",
    "    get_imgaug_transform,\n",
    "    get_loss_factories,\n",
    "    get_model,\n",
    "    get_callbacks,\n",
    "    calculate_train_batches,\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "\n",
    "from funcs import (\n",
    "    get_keypoint_names,\n",
    "    get_videos_in_dir,\n",
    "    mask_df,\n",
    "    closest_multiple128,\n",
    "    dlc2lp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**To create a LP dataset, follow these steps:**\n",
    "\n",
    "- [Step 1: Converting the DLC project to Lightning Pose format](#Step-1:-Converting-the-DLC-project-to-Lightning-Pose-format)\n",
    "- [Step 2: Update the yaml config file](#Step-2:-update-the-yaml-config-file)\n",
    "- [Step 3: Check if the training data exist](#Step-3:-Check-if-the-training-data-exist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Converting the DLC project to Lightning Pose format\n",
    "\n",
    "**DeeplabCut assumes the following project directory structure:**\n",
    "```console\n",
    "    /path/to/DLC_project/\n",
    "      ├── labeled-data/\n",
    "      └── videos/\n",
    "```\n",
    "* `labeled-data`: This directory stores the frames used to create the DLC training dataset. Frames from different videos are stored in separate subdirectories. Each frame has a filename related to the temporal index within the corresponding video, which allows the user to trace every frame back to its origin.\n",
    "\n",
    "* `videos`: Directory of video links or videos. \n",
    "\n",
    "An example of DLC project is `/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lightning Pose assumes the following [project directory structure](https://lightning-pose.readthedocs.io/en/latest/source/user_guide/directory_structure.html)**, as in the example dataset\n",
    "provided in [mirror-mouse](https://github.com/danbider/lightning-pose/tree/main/data/mirror-mouse-example).\n",
    "```console\n",
    "    /path/to/LP_project/\n",
    "      ├── <LABELED_DATA_DIR>/\n",
    "      ├── <VIDEO_DIR>/\n",
    "      └── <YOUR_LABELED_FRAMES>.csv\n",
    "```\n",
    "* `<YOUR_LABELED_FRAMES>.csv`: a table with keypoint labels (rows: frames; columns: keypoints). \n",
    "Note that this file can take any name, and needs to be specified in the config file under \n",
    "`data.csv_file`.\n",
    "\n",
    "* `<LABELED_DATA_DIR>/`: contains images that correspond to the labels, and can include subdirectories.\n",
    "The directory name, any subdirectory names, and image names are all flexible, as long as they are\n",
    "consistent with the first column of `<YOUR_LABELED_FRAMES>.csv`.\n",
    "\n",
    "* `<VIDEO_DIR>/`: when training semi-supervised models, the videos in this directory will be used \n",
    "for computing the unsupervised losses. This directory can take any name, and needs to be specified \n",
    "in the config file under `data.video_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert a DLC project to LP format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# set up the path to DLC project, LP training data and LP outputs \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# use Han's data\n",
    "# set up the path to the DLC project\n",
    "scorer_name  = \"Han_behavior_data\"   \n",
    "project_name = \"Foraging_Bot-Han_Lucas-2022-04-27\" \n",
    "DLC_data_dir = os.path.join(\"/root/capsule/data/s3_video/DLC_projects\", project_name)\n",
    "\n",
    "# columns_to_pick indicates which keypoints included in training\n",
    "# if columns_to_pick is null, using all the keypoints for training \n",
    "columns_to_pick = []\n",
    "# assume dlc format\n",
    "header_rows = [0, 1, 2]\n",
    "\n",
    "\n",
    "# # use VBN data\n",
    "# scorer_name  = \"VBN_behavior_DLC\"\n",
    "# scorer_name  = \"VBN_behavior_DLC_test\"\n",
    "# project_name = \"face\"\n",
    "# DLC_data_dir = os.path.join(\"/root/capsule/data/vbn_dlc_all_4\", project_name)\n",
    "# columns_to_pick = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 24, 27, 28, 39]\n",
    "# header_rows = [0, 1, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_dir: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/\n",
      "Video names: ['bottom_face_888-0000', 'bottom_face_6-0000', 'bottom_face_670-0000', 'bottom_face_115-0000', 'bottom_face_49-0000', 'bottom_face_533-0000', 'bottom_face_1-0000', 'bottom_face_186-0000', 'bottom_face_593-0000', 'bottom_face_110-0000', 'bottom_face_857-0000', 'bottom_face_37-0000', 'bottom_face_484-0000', 'bottom_face_5-0000', 'bottom_face_521-0000', 'bottom_face_718-0000', 'bottom_face_137-0000', 'bottom_face_164-0000', 'bottom_face_65-0000', 'bottom_face_116-0000', 'bottom_face_171-0000', 'bottom_face_205-0000', 'bottom_face_866-0000', 'bottom_face_114-0000', 'bottom_face_20-0000', 'bottom_face_101-0000', 'bottom_face_861-0000', 'bottom_face_41-0000', 'bottom_face_197-0000']\n",
      "The number of videos: 29\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# get the videos which contain labeled frames\n",
    "# ---------------------------------------\n",
    "DLC_video_dir   = os.path.join(DLC_data_dir, 'videos/')\n",
    "\n",
    "DLC_video_files = get_videos_in_dir(DLC_video_dir)\n",
    "video_names = []\n",
    "for video_file in DLC_video_files:\n",
    "    video_names.append(video_file.split('/')[-1][:-4])\n",
    "print(f\"Video names: {video_names}\")\n",
    "print(f\"The number of videos: {len(video_names)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# select videos you want to train on\n",
    "# ---------------------------------------\n",
    "\n",
    "# for Han's data\n",
    "videos_picked = ['bottom_face_1-0000',\n",
    "                 'bottom_face_5-0000',\n",
    "                 'bottom_face_20-0000',\n",
    "                 'bottom_face_49-0000',\n",
    "                 'bottom_face_116-0000',\n",
    "                 'bottom_face_484-0000',\n",
    "                 'bottom_face_533-0000',\n",
    "                 'bottom_face_670-0000']\n",
    "\n",
    "# # for VBN data\n",
    "# videos_picked = ['1128520325_585326_20210915.face', \n",
    "#                  '1122903357_570302_20210818.face', \n",
    "#                  '1052533639_530862_20200924.face', \n",
    "#                  ]\n",
    "\n",
    "# if videos_picked is null, using all labels instead of only using the labels in selected videos\n",
    "if len(videos_picked) == 0:\n",
    "    videos_picked = video_names\n",
    "num_videos = len(videos_picked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save LP model to /root/capsule/results\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# set up the path to the LP data and outputs\n",
    "# ---------------------------------------\n",
    "# LP_data_dir = os.path.join(\"/root/capsule/results\",\n",
    "#                             project_name)\n",
    "# Path(LP_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LP_data_dir = os.path.abspath(\"../results\")\n",
    "print(f\"Save LP model to {LP_data_dir}\")\n",
    "LP_output_dir = os.path.join(LP_data_dir, \"outputs\")\n",
    "Path(LP_output_dir).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/capsule/results\n",
      "lrwxrwxrwx. 1 root root 8 Aug 26 16:50 /root/capsule/results -> /results\n"
     ]
    }
   ],
   "source": [
    "print(LP_data_dir)\n",
    "! ls -la $LP_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting a DLC project to LP format .....\n",
      "\n",
      "Converting DLC project located at /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27 to LP project located at /root/capsule/results\n",
      "Start generating <YOUR_LABELED_FRAMES>.csv!\n",
      "---- bottom_face_1-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_1-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_5-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_5-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_20-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_20-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_49-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_49-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_116-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_116-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_484-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_484-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_533-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_533-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_670-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_670-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "Finish generating <YOUR_LABELED_FRAMES>.csv!\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Start generating <VIDEO_DIR>/!\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_1-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_1-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_1-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_1-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_1-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_5-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_5-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_5-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_5-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_5-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_20-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_20-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_20-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_20-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_20-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_49-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_49-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_49-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_49-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_49-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_116-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_116-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_116-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_116-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_116-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_484-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_484-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_484-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_484-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_484-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_533-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_533-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_533-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_533-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_533-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_670-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/results/videos/bottom_face_670-0000.mp4\n",
      "Moviepy - Building video /root/capsule/results/videos/bottom_face_670-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/results/videos/bottom_face_670-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/results/videos/bottom_face_670-0000.mp4\n",
      "Finish generating <VIDEO_DIR>/!\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Start generating <LABELED_DATA_DIR>/!\n",
      "Finish generating <LABELED_DATA_DIR>/!\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "The number of labeled frames: 154\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Convert DLC project to LP format\n",
    "# ---------------------------------------\n",
    "print(f\"Converting a DLC project to LP format .....\")\n",
    "\n",
    "# Call dlc2lp() to generate the LP dataset with the following directory struture\n",
    "#     /path/to/LP_project/\n",
    "#       ├── <LABELED_DATA_DIR>/\n",
    "#       ├── <VIDEO_DIR>/\n",
    "#       └── <YOUR_LABELED_FRAMES>.csv\n",
    "\n",
    "# set up the path to <YOUR_LABELED_FRAMES>.csv\n",
    "# LP_labels_file_all contains all the bodyparts\n",
    "LP_labels_file_all = os.path.join(LP_data_dir, f\"CollectedData.csv\")\n",
    "\n",
    "model_name = f\"trained_with_{num_videos}videos\" # the name for LP model\n",
    "df_data = dlc2lp(DLC_data_dir, \n",
    "               LP_data_dir, \n",
    "               model_name, \n",
    "               LP_labels_file_all,\n",
    "               videos_picked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/capsule/results\n",
      "lrwxrwxrwx. 1 root root 8 Aug 26 16:50 /root/capsule/results -> /results\n"
     ]
    }
   ],
   "source": [
    "print(LP_data_dir)\n",
    "! ls -la $LP_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoint names: ['tongueTip', 'tongueLeftFront', 'tongueRightFront', 'tongueLeftBack', 'tongueRightBack', 'LickportLeft', 'LickportRight', 'nosetip', 'jaw', 'pawL', 'pawR', 'WLup', 'WLmid', 'WLbot', 'WRup', 'WRmid', 'WRbot'], 17\n",
      "\n",
      "Selected keypoints: ['tongueTip', 'tongueLeftFront', 'tongueRightFront', 'tongueLeftBack', 'tongueRightBack', 'LickportLeft', 'LickportRight', 'nosetip', 'jaw', 'pawL', 'pawR', 'WLup', 'WLmid', 'WLbot', 'WRup', 'WRmid', 'WRbot'], 17\n",
      "\n",
      "Save masked LP label file to /root/capsule/results/CollectedData.masked.csv\n",
      "\n",
      "The LP label file for training: /root/capsule/results/CollectedData.masked.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# select keypoints to train\n",
    "# we could include all keypoints or certain keypoints for training \n",
    "# LP_labels_file_all: <.csv> inludes all keypoints\n",
    "# LP_labels_file_masked: <.masked.csv> only contains the columns of selected keypoints\n",
    "# ---------------------------------------\n",
    "\n",
    "# get the keypoints names\n",
    "keypoint_names = get_keypoint_names(LP_labels_file_all, header_rows)\n",
    "print(f\"Keypoint names: {keypoint_names}, {len(keypoint_names)}\")\n",
    "\n",
    "# select keypoints for training\n",
    "if len(columns_to_pick) == 0:\n",
    "    keypoint_to_pick = keypoint_names # if columns_to_pick is null, using all the keypoints for training \n",
    "else:\n",
    "    keypoint_to_pick = [ keypoint_names[col] for col in columns_to_pick ]\n",
    "print(f\"\\nSelected keypoints: {keypoint_to_pick}, {len(keypoint_to_pick)}\")\n",
    "\n",
    "# extract the columns of selected keypoints\n",
    "df_data_masked = mask_df(LP_labels_file_all, header_rows, keypoint_to_pick)\n",
    "\n",
    "# save masked annotation files which only contains the labels of the selected keypoints\n",
    "LP_labels_file_masked = LP_labels_file_all.replace(\".csv\", \".masked.csv\")\n",
    "df_data_masked.to_csv(LP_labels_file_masked)\n",
    "print(f\"\\nSave masked LP label file to {LP_labels_file_masked}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# set up the LP annotation file\n",
    "# ---------------------------------------\n",
    "LP_labels_file = LP_labels_file_masked \n",
    "# LP_labels_file = LP_labels_file_all\n",
    "print(f\"\\nThe LP label file for training: {LP_labels_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/results/outputs\n",
      "total 0\n",
      "\n",
      "Store training and testing results to /root/capsule/results/outputs\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# change the working dir\n",
    "# ---------------------------------------\n",
    "# LP_output_dir = os.getcwd()\n",
    "%pwd\n",
    "%cd $LP_output_dir\n",
    "%pwd\n",
    "! ls -lt $LP_output_dir\n",
    "print(f\"\\nStore training and testing results to {LP_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: update the yaml config file\n",
    "\n",
    "After generating LP dataset, you will need to update your config file with the correct paths. This file points to data directories, defines the type of models to fit, and specifies a wide range of hyperparameters. The default configuration file at [here](https://github.com/danbider/lightning-pose/blob/main/scripts/configs/config_default.yaml) enumerates all possible hyperparameters needed for building and training a model. See [here](https://lightning-pose.readthedocs.io/en/latest/source/user_guide/config_file.html) for more information.\n",
    "\n",
    "**To create the yaml config file, follow these steps:**\n",
    "* [(1) update the path to the training data](#(1)-update-the-path-to-the-training-data)\n",
    "* [(2) update the testing video path](#(2)-update-the-testing-video-path)\n",
    "* [(3) update the image dimensions ](#(3)-update-the-image-dimensions)\n",
    "* [(4) update the keypoint info](#(4)-update-the-keypoint-info)\n",
    "* [(5) set up training parameters](#(5)-set-up-training-parameters)\n",
    "* [(6) set up unsupervised losses](#(6)-set-up-unsupervised-losses)\n",
    "<!-- * [(7) set up the fully-supervised training](#(7)-set-up-fully-supervised-training) -->\n",
    "* [(7) (Optional) transfer learning: load a pretrained model](#(7)-(Optional)-Transfer-learning:-load-a-pretrained-model)\n",
    "* [(8) save the updated LP config file](#(8)-save-the-updated-LP-config-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    " Below is a list of some commonly modified arguments in a LP config file related to model architecture/training. When training a model on a new dataset, you should copy/paste the default config and update the\n",
    "arguments to match your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - data.csv_file: location of labels\n",
    "- data.video_dir: location of unlabeled videos\n",
    "- data.num_keypoints: total number of keypoints\n",
    "- data.columns_for_singleview_pca: list of indices of keypoints used for pca singleview loss\n",
    "<br/><br/>\n",
    "\n",
    "- training.train_batch_size (default: `16`) - batch size for labeled data\n",
    "- training.train_prob (default: `0.8`) - fraction of labeled data used for training\n",
    "- training.val_prob (default: `0.1`) - fraction of labeled data used for validation (remaining used for test)\n",
    "- training.min_epochs (default: `300`)\n",
    "- training.max_epochs (default: `750`)\n",
    "<br/><br/>\n",
    "\n",
    "- model.model_type (default: `heatmap`)\n",
    "  - regression: model directly outputs an (x, y) prediction for each keypoint; not recommended\n",
    "  - heatmap: model outputs a 2D heatmap for each keypoint\n",
    "  - heatmap_mhcrnn: the \"multi-head convolutional RNN\", this model takes a temporal window of\n",
    "    frames as input, and outputs two heatmaps: one \"context-aware\" and one \"static\". The prediction\n",
    "    with the highest confidence is automatically chosen. Must also set `model.do_context=True`.\n",
    "- model.losses_to_use (default: `[]`) - this argument relates to the unsupervised losses. An empty\n",
    "  list indicates a fully supervised model. Each element of the list corresponds to an unsupervised\n",
    "  loss. For example,\n",
    "  `model.losses_to_use=[pca_multiview,temporal]` will fit both a pca_multiview loss and a temporal\n",
    "  loss. Options include:\n",
    "  - pca_multiview: penalize inconsistencies between multiple camera views\n",
    "  - pca_singleview: penalize implausible body configurations\n",
    "  - temporal: penalize large temporal jumps\n",
    "<br/><br/>\n",
    "\n",
    "- eval.test_videos_directory - str with an absolute path to a directory containing videos for prediction.\n",
    "- eval.confidence_thresh_for_vid (default: `0.9`) - confidence threshold for plotting a vid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LP default config file, and update parameter wrt your own behavior data\n",
    "LP_config_template = \"/code/configs/config_default.yaml\"\n",
    "with open(LP_config_template, 'r') as file:\n",
    "    param_updated = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) update the path to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute path to a directory containing LP labeled frames. \n",
    "# Frames from different videos are stored in separate subdirectories.\n",
    "param_updated['data'][\"data_dir\"] = LP_data_dir \n",
    "\n",
    "# absolute path to the LP annotation file. \n",
    "# Each frame has a filename related to the temporal index within the corresponding video, \n",
    "# which allows the user to trace every frame back to its origin.\n",
    "param_updated['data'][\"csv_file\"] = LP_labels_file\n",
    "\n",
    "# absolute path to a directory containing videos for training\n",
    "LP_video_dir = os.path.join(LP_data_dir, 'videos/')\n",
    "param_updated['data'][\"video_dir\"] = LP_video_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) update the testing video path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute path to a directory containing videos for prediction.\n",
    "param_updated['eval']['test_videos_directory'] = LP_video_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) update the image dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df['scorer'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path to the first labeled frame:/root/capsule/results/labeled-data/bottom_face_1-0000/img0026.png\n"
     ]
    }
   ],
   "source": [
    "# load ground truth\n",
    "labels_df = pd.read_csv(LP_labels_file)\n",
    "\n",
    "# get the absolute path to the first labeled frame \n",
    "frame_1st = os.path.join( LP_data_dir, labels_df['scorer'].to_list()[2])\n",
    "print(f\"The path to the first labeled frame:{frame_1st}\")\n",
    "\n",
    "# get the image dimension\n",
    "image = Image.open(frame_1st).convert(\"RGB\")\n",
    "# set up resize dimension, LP requires its a multiple of 128 to accelerate training.\n",
    "# Optional: limit image size to 640 to avoid OOM\n",
    "param_updated['data'][\"image_resize_dims\"][\"width\"]  = closest_multiple128(image.size[0]) if image.size[0] < 640 else 640\n",
    "param_updated['data'][\"image_resize_dims\"][\"height\"] = closest_multiple128(image.size[1]) if image.size[1] < 640 else 640\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) update the keypoint info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint names: ['tongueTip', 'tongueLeftFront', 'tongueRightFront', 'tongueLeftBack', 'tongueRightBack', 'LickportLeft', 'LickportRight', 'nosetip', 'jaw', 'pawL', 'pawR', 'WLup', 'WLmid', 'WLbot', 'WRup', 'WRmid', 'WRbot']\n",
      "The number of keypoints: 17\n"
     ]
    }
   ],
   "source": [
    "# get the keypoints names\n",
    "keypoint_names = get_keypoint_names(LP_labels_file, header_rows)\n",
    "num_bodyparts  = len(keypoint_names)\n",
    "print(f\"keypoint names: {keypoint_names}\")\n",
    "print(f\"The number of keypoints: {num_bodyparts}\")\n",
    "\n",
    "param_updated['data'][\"num_keypoints\"] = num_bodyparts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) set up training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training frames include both visible and occluded keypoint, Lightning pose will output the confidence value always close to 1 for occluded keypoints (i.e, tongue). \n",
    "To address this issue, LP has a non-default option that includes missing data in the loss by comparing the predicted heatmap to a uniform heatmap. \n",
    "To set the non-default option, add the following option to your config yaml file (under the \"training\" key):\n",
    "```\n",
    "training:\n",
    "    uniform_heatmaps_for_nan_keypoints: true\n",
    "```\n",
    "See [here](https://lightning-pose.readthedocs.io/en/latest/source/faqs.html#faq-nan-heatmaps) for more information about why the network produce high confidence values for keypoints even when they are occluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_updated[\"training\"][\"uniform_heatmaps_for_nan_keypoints\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) set up unsupervised losses \n",
    "For a detailed mathematical description of the losses, see the [Lightning Pose paper](https://www.biorxiv.org/content/10.1101/2023.04.28.538703v1). \n",
    "\n",
    "See [here](https://lightning-pose.readthedocs.io/en/latest/source/user_guide_advanced/unsupervised_losses.html) for more details on how to use unsupervised losses (i.e., Temporal continuity, Pose plausibility and Multiview consistency) and set up hyperparameters.\n",
    "\n",
    "If you plan to use the PCA losses (Pose PCA or multiview PCA) then all training images must be the same size, otherwise the PCA subspace will erroneously contain variance related to image size, see [here](https://github.com/danbider/lightning-pose/blob/47ee289110fb2ef2519091b49a5658fab07b4bf4/docs/source/user_guide/config_file.rst#L28) for more details.\n",
    "\n",
    "\n",
    "To apply unsupervised losses on unlabeled video data, model.losses_to_use must be non-empty (which indicates a fully supervised model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_updated['model'][\"losses_to_use\"] = [\"pca_singleview\"] # or multiple losses: [temporal,pca_singleview]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns_for_singleview_pca: list of indices of keypoints used for pca singleview loss.\n",
    "\n",
    "Ensure the number of samples is greater than the obervation dimensions (have more rows than columns after doing nan filtering)!\n",
    "Since each keypoint is 2-dimensional (x, y coords), if there are K keypoints labeled on each frame then each pose \n",
    "is described by a 2K-dimensional vector. Therefore, at least 2K frames need to be labeled to compute the PCA subspace.\n",
    "\n",
    "If the error massage \"cannot fit PCA with N samples < M observation dimensions\" occures, \n",
    "reselect or reduce the columns_for_singleview_pca or enlarge the training data size.\n",
    "\n",
    "It is up to the user to select which keypoints are included in the Pose plausibility loss. \n",
    "Including static keypoints (e.g. those marking a corner of an arena) are generally not helpful. \n",
    "Also be careful to not include keypoints that are often occluded, like the tongue. If these keypoints \n",
    "are included the loss will try to localize them even when they are occluded, which might be unhelpful if you \n",
    "want to use the confidence of the outputs as a lick detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.tongueTip\n",
      "1.tongueLeftFront\n",
      "2.tongueRightFront\n",
      "3.tongueLeftBack\n",
      "4.tongueRightBack\n",
      "5.LickportLeft\n",
      "6.LickportRight\n",
      "7.nosetip\n",
      "8.jaw\n",
      "9.pawL\n",
      "10.pawR\n",
      "11.WLup\n",
      "12.WLmid\n",
      "13.WLbot\n",
      "14.WRup\n",
      "15.WRmid\n",
      "16.WRbot\n"
     ]
    }
   ],
   "source": [
    "for ind, name in enumerate(keypoint_names):\n",
    "    print(f\"{ind}.{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_updated['data'][\"columns_for_singleview_pca\"] = [ i for i in range(num_bodyparts) ] \n",
    "# The numbers used should correspond to the order of the keypoints in the labeled csv file. \n",
    "param_updated['data'][\"columns_for_singleview_pca\"] = [ 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) (Optional) Transfer learning: load a pretrained model\n",
    "\n",
    "model.checkpoint: to initialize weights from an existing checkpoint, update this parameter to the absolute path of a pytorch .ckpt file,\n",
    "see [here](https://github.com/danbider/lightning-pose/blob/47ee289110fb2ef2519091b49a5658fab07b4bf4/docs/source/user_guide/config_file.rst#L28).\n",
    "If you would like to train a model from scratch, just comment the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model_path = \"/root/capsule/data/pretrained_models/Han_behavior_data/Foraging_Bot-Han_Lucas-2022-04-27/epoch=99-step=800_8videos.ckpt\"\n",
    "# param_updated['model'][\"checkpoint\"] = pretrained_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) save the updated LP config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute path to save updated LP config file\n",
    "LP_config_file = os.path.join(LP_data_dir,  \n",
    "                              'training.config.yaml') \n",
    "\n",
    "# save  \n",
    "with open(LP_config_file, 'w') as yaml_file:\n",
    "    yaml.dump(param_updated, yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/capsule/results/training.config.yaml'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LP_config_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check if the training data exist\n",
    "\n",
    "```console\n",
    "    /path/to/LP_project/\n",
    "      ├── <LABELED_DATA_DIR>/\n",
    "      ├── <VIDEO_DIR>/\n",
    "      └── <YOUR_LABELED_FRAMES>.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP training data: /root/capsule/results\n",
      "LP training videos: /root/capsule/results/videos/\n",
      "LP annotation file: /root/capsule/results/CollectedData.masked.csv\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "cfg = OmegaConf.load(LP_config_file)\n",
    "\n",
    "# print(\"Our Hydra config file:\")\n",
    "# pretty_print_cfg(cfg)\n",
    "\n",
    "# path handling for the dataset\n",
    "data_dir, video_dir = return_absolute_data_paths(data_cfg=cfg.data)\n",
    "\n",
    "# <LABELED_DATA_DIR>: cfg.data.data_dir, the absolute path to the labeled frames\n",
    "assert os.path.isdir(cfg.data.data_dir), \"data_dir not a valid directory\"\n",
    "\n",
    "# <VIDEO_DIR>: cfg.data.video_dir, the absolute path to the videos\n",
    "assert os.path.isdir(cfg.data.video_dir), \"video_dir not a valid directory\"\n",
    "\n",
    "# <YOUR_LABELED_FRAMES>.csv: cfg.data.csv_file, the absolute path to the annotation file\n",
    "df_tmp = pd.read_csv(cfg.data.csv_file, header=header_rows, index_col=0)\n",
    "for img in df_tmp.index:\n",
    "    assert os.path.exists(os.path.join(cfg.data.data_dir, img))\n",
    "    \n",
    "print(f\"LP training data: {data_dir}\")\n",
    "print(f\"LP training videos: {video_dir}\")\n",
    "print(f\"LP annotation file: {cfg.data.csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D47Ko2u-R5Ko"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/results/outputs\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xw6_WU3dVC30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dlc image augmentation pipeline\n",
      "\n",
      " Initializing a HeatmapTracker instance.\n",
      "Downloading: \"https://download.openmmlab.com/mmpose/animal/resnet/res50_ap10k_256x256-35760eb8_20211029.pth\" to /root/.cache/torch/hub/checkpoints/res50_ap10k_256x256-35760eb8_20211029.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130M/130M [00:03<00:00, 45.0MB/s] \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# build dataset, model, and trainer\n",
    "\n",
    "# make training short for a demo (we usually do 300)\n",
    "# (approx 2 mins for training Han's data using fully-supervised learning with epoch=55)\n",
    "# (approx 6 mins for training Han's data using semi-supervised learning (losses_to_use: ['pca_singleview']) with epoch=55)\n",
    "cfg.training.min_epochs = 100\n",
    "cfg.training.max_epochs = 200\n",
    "cfg.training.batch_size = 8\n",
    "\n",
    "# build imgaug transform\n",
    "imgaug_transform = get_imgaug_transform(cfg=cfg)\n",
    "\n",
    "# build dataset\n",
    "dataset = get_dataset(cfg=cfg, data_dir=data_dir, imgaug_transform=imgaug_transform)\n",
    "\n",
    "# build datamodule; breaks up dataset into train/val/test\n",
    "data_module = get_data_module(cfg=cfg, dataset=dataset, video_dir=video_dir)\n",
    "\n",
    "# build loss factory which orchestrates different losses\n",
    "loss_factories = get_loss_factories(cfg=cfg, data_module=data_module)\n",
    "\n",
    "# build model\n",
    "model = get_model(cfg=cfg, data_module=data_module, loss_factories=loss_factories)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Set up and run training\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# logger\n",
    "logger = pl.loggers.TensorBoardLogger(\"tb_logs\", name=cfg.model.model_name)\n",
    "\n",
    "# early stopping, learning rate monitoring, model checkpointing, backbone unfreezing\n",
    "callbacks = get_callbacks(cfg)\n",
    "\n",
    "# calculate number of batches for both labeled and unlabeled data per epoch\n",
    "limit_train_batches = calculate_train_batches(cfg, dataset)\n",
    "\n",
    "# set up trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=cfg.training.max_epochs,\n",
    "    min_epochs=cfg.training.min_epochs,\n",
    "    check_val_every_n_epoch=cfg.training.check_val_every_n_epoch,\n",
    "    log_every_n_steps=cfg.training.log_every_n_steps,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    limit_train_batches=limit_train_batches,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type        | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | backbone          | Sequential  | 23.5 M | train\n",
      "1 | loss_factory      | LossFactory | 0      | train\n",
      "2 | upsampling_layers | Sequential  | 81.0 K | train\n",
      "----------------------------------------------------------\n",
      "23.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.6 M    Total params\n",
      "94.356    Total estimated model params size (MB)\n",
      "154       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled images in the full dataset (train+val+test): 154\n",
      "Dataset splits -- train: 147, val: 7, test: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365e44050de04049bf7e5b6867ae0955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training duration: 0:07:52.602906\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "# train model!\n",
    "# Train the model \n",
    "# (approx 2 mins for training Han's data using fully-supervised learning with epoch=55)\n",
    "# (approx 6 mins for training Han's data using semi-supervised learning (losses_to_use: ['pca_singleview']) with epoch=55)\n",
    "trainer.fit(model=model, datamodule=data_module)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nTraining duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/capsule/results/outputs'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LP_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 276888\n",
      "-rw-r--r--. 1 root root 283529831 Aug 26 17:00 'epoch=169-step=1700-best.ckpt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Once training has completed, use the checkpoint that corresponds to the best performance you found during the training process.\n",
    "# a checkpoint: a version of the model. \n",
    "# check the trained model\n",
    "! ls -lt \"/root/capsule/results/outputs/tb_logs/test/version_0/checkpoints/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy best_model to the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydra output directory: /results/outputs\n",
      "Best checkpoint: /results/outputs/tb_logs/test/version_0/checkpoints/epoch=169-step=1700-best.ckpt\n",
      "Copying /results/outputs/tb_logs/test/version_0/checkpoints/epoch=169-step=1700-best.ckpt to /root/capsule/results\n",
      "Finish copying model\n"
     ]
    }
   ],
   "source": [
    "hydra_output_directory = os.getcwd()\n",
    "print(f\"Hydra output directory: {hydra_output_directory}\")\n",
    "\n",
    "# get best ckpt\n",
    "best_ckpt = os.path.abspath(trainer.checkpoint_callback.best_model_path)\n",
    "print(f\"Best checkpoint: {best_ckpt}\")\n",
    "\n",
    "# check if best_ckpt is a file\n",
    "if not os.path.isfile(best_ckpt):\n",
    "    raise FileNotFoundError(\"Cannot find checkpoint. Have you trained for too few epochs?\")\n",
    "\n",
    "print(f\"Copying {best_ckpt} to {LP_data_dir}\")\n",
    "shutil.copy(best_ckpt, LP_data_dir)\n",
    "print(f\"Finish copying model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) delete outputs/ folder \n",
    "Below cell is used to delete outputs/ folder to only keep one .ckpt file in the results/ for a pretrained model data asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(hydra_output_directory):\n",
    "#     shutil.rmtree(hydra_output_directory)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
