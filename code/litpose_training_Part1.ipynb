{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXyo-JJDpEJV"
   },
   "source": [
    "# ⚡ Generate a lightning-pose (LP) dataset and train a LP model ⚡\n",
    "This notebook shows how to convert the DeepLabCut (DLC) project to Lightning-Pose (LP) format and train LP model step by step. \n",
    "\n",
    "Here, we take Han's bottom-view DLC project and VBN project for example.\n",
    "<!-- * [Environment setup](#Environment-setup) -->\n",
    "* [Data preparation](#Data-preparation)\n",
    "<!-- * [Monitor optimization in real time (via TensorBoard UI)](#Monitor-training) -->\n",
    "* [Training](#Training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Materials for Lightning Pose:</b>\n",
    "    \n",
    "- [Paper](https://www.biorxiv.org/content/10.1101/2023.04.28.538703v1) shows a detailed mathematical description of the LP algorithm.\n",
    "\n",
    "- [GitHub](https://github.com/danbider/lightning-pose) and [Documentation](https://lightning-pose.readthedocs.io/en/latest/index.html) show how to implement LP.\n",
    "\n",
    "- Reference for this notebook at [here](https://github.com/danbider/lightning-pose/blob/7da5b5e701cb315ffd6d3ac8847191ee6715c46e/scripts/litpose_training_demo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Make sure to attach the data asset:</b>\n",
    "\n",
    "To do so, go to data/:\n",
    "* click the \"Manage Data Assets\" button \n",
    "* for Han's data: search \"han_video_s3\"\n",
    "* for VBN data: search \"vbn_dlc_all_4\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VXYyD0YFZfpE"
   },
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import os\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from lightning_pose.utils import pretty_print_str, pretty_print_cfg\n",
    "from lightning_pose.utils.io import (\n",
    "    check_video_paths,\n",
    "    return_absolute_data_paths,\n",
    "    return_absolute_path,\n",
    ")\n",
    "from lightning_pose.utils.predictions import predict_dataset\n",
    "from lightning_pose.utils.scripts import (\n",
    "    export_predictions_and_labeled_video,\n",
    "    get_data_module,\n",
    "    get_dataset,\n",
    "    get_imgaug_transform,\n",
    "    get_loss_factories,\n",
    "    get_model,\n",
    "    get_callbacks,\n",
    "    calculate_train_batches,\n",
    "    compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**To create a LP dataset, follow these steps:**\n",
    "\n",
    "- [Step 1: Converting the DLC project to Lightning Pose format](#Step-1:-Converting-the-DLC-project-to-Lightning-Pose-format)\n",
    "- [Step 2: Update the yaml config file](#Step-2:-update-the-yaml-config-file)\n",
    "- [Step 3: Check if the training data exist](#Step-3:-Check-if-the-training-data-exist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from moviepy.editor import VideoFileClip\n",
    "import moviepy.editor as moviepy\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def get_keypoint_names(csv_file: str, header_rows: List[int]) -> List[str]:\n",
    "    \"\"\" get the bodypart names given the .csv file\n",
    "    \n",
    "    Args:\n",
    "        csv_file: the prediction/evaluation .csv file\n",
    "        header_rows: multiindex header of the .csv file\n",
    "\n",
    "    Returns:\n",
    "        list of bodypart names\n",
    "    \"\"\"\n",
    "    if os.path.exists(csv_file):\n",
    "        csv_data = pd.read_csv(csv_file, header=header_rows)\n",
    "        # collect marker names from multiindex header\n",
    "        if header_rows == [1, 2] or header_rows == [0, 1]:\n",
    "            keypoint_names = [b[0] for b in csv_data.columns if b[1] == \"x\"]\n",
    "        elif header_rows == [0, 1, 2]:\n",
    "            keypoint_names = [b[1] for b in csv_data.columns if b[2] == \"x\"]\n",
    "    else:\n",
    "        # keypoint_names = [\"bp_%i\" % n for n in range(cfg.data.num_targets // 2)]\n",
    "        keypoint_names = []\n",
    "        print(\"keypoint_names do not exist!!!\")\n",
    "    return keypoint_names\n",
    "\n",
    "\n",
    "def get_videos_in_dir(video_dir: str, return_mp4_only: bool=False) -> List[str]:\n",
    "    \"\"\"Get all video files in directory given allowed formats \"\"\"\n",
    "    # gather videos to process\n",
    "    print(f\"video_dir: {video_dir}\")\n",
    "    assert os.path.isdir(video_dir)\n",
    "\n",
    "    allowed_formats = (\".mp4\", \".avi\", \".mov\")\n",
    "    if return_mp4_only == True:\n",
    "        allowed_formats = \".mp4\"\n",
    "    video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith(allowed_formats)]\n",
    "\n",
    "    if len(video_files) == 0:\n",
    "        raise IOError(\"Did not find any valid video files in %s\" % video_dir)\n",
    "    return video_files\n",
    "\n",
    "\n",
    "def convert_header_to_dlc(df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\" Convert the header of dataframe to DLC format\"\"\"\n",
    "    df_arry = df.to_numpy()\n",
    "    # keypoint_names = [b[0] for b in df.columns if b[1] == \"x\"]\n",
    "    keypoint_names = df.columns.get_level_values(\"bodyparts\").unique()\n",
    "    print(f\"The number of keypoint: {len(keypoint_names)}\" )\n",
    "    # model_name = 'DLC_resnet50'\n",
    "    pdindex = make_labels_dlc_index(model_name, keypoint_names)\n",
    "    df_dlc_index = pd.DataFrame(df_arry, columns=pdindex, index = df.index)\n",
    "\n",
    "    return df_dlc_index\n",
    "\n",
    "\n",
    "def make_labels_dlc_index(model_name: str, keypoint_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Create the MultiIndex with DLC format \"\"\"\n",
    "    # xyl_labels = [\"x\", \"y\", \"likelihood\"] # for prediction\n",
    "    xyl_labels = [\"x\", \"y\"] # for ground truth\n",
    "    pdindex = pd.MultiIndex.from_product(\n",
    "        [[\"%s_tracker\" % model_name], keypoint_names, xyl_labels],\n",
    "        names=[\"scorer\", \"bodyparts\", \"coords\"],\n",
    "    )\n",
    "    \n",
    "    return pdindex\n",
    "\n",
    "\n",
    "def mask_df(file: str, header_rows: List[int], bodyparts: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Select the columns in bodyparts, return the masked dataframe \"\"\"\n",
    "    df = pd.read_csv(file, header=header_rows, index_col=0) \n",
    "    mask = df.columns.get_level_values(\"bodyparts\").isin(bodyparts)\n",
    "    df_masked = df.loc[:, mask]\n",
    "    \n",
    "    return df_masked\n",
    "\n",
    "\n",
    "def closest(K: int) -> int:\n",
    "    \"\"\" Find the nearest multiple of 128 to K.\n",
    "    LP requires the dimension of image dimension is a multiple of 128 to accelerate training.\n",
    "    \"\"\"\n",
    "    if K <= 128:\n",
    "        return 128\n",
    "    elif K >= 1280:\n",
    "        return 1280\n",
    "    else:\n",
    "        return int(round(K/128))*128\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlc2lp(\n",
    "    dlc_dir: str, \n",
    "    lp_dir: str, \n",
    "    model_name: str, \n",
    "    save_lp_csv: str, \n",
    "    videos_picked: List[str]) -> pd.DataFrame:\n",
    "    '''Converting DLC project located at dlc_dir to LP project located at lp_dir\n",
    "    Call dlc2lp() to generate the LP dataset with the following directory struture\n",
    "     /path/to/LP_project/\n",
    "       ├── <LABELED_DATA_DIR>/\n",
    "       ├── <VIDEO_DIR>/\n",
    "       └── <YOUR_LABELED_FRAMES>.csv\n",
    "       \n",
    "    Args:\n",
    "        dlc_dir: path to DLC project\n",
    "        lp_dir: path to save LP project\n",
    "        model_name: LP model name\n",
    "        save_lp_csv: path to save the LP labels .csv file\n",
    "        videos_picked: videos of interest used for generating the LP labels .csv file\n",
    "        \n",
    "    Return:\n",
    "        pd.DataFrame: concatenated labels\n",
    "    '''\n",
    "    print(f\"\\nConverting DLC project located at {dlc_dir} to LP project located at {lp_dir}\")\n",
    "    \n",
    "    # check provided DLC path exists\n",
    "    if not os.path.exists(dlc_dir):\n",
    "        raise NotADirectoryError(f\"did not find the directory {dlc_dir}\")\n",
    "\n",
    "    # check paths are not the same\n",
    "    if dlc_dir == lp_dir:\n",
    "        raise NameError(f\"dlc_dir and lp_dir cannot be the same\")\n",
    "  \n",
    "    # check videos_picked\n",
    "    if len(videos_picked) == 0:\n",
    "        raise ValueError(f\"videos_picked cannot be null\")\n",
    "        \n",
    "    # find all labeled data in DLC project\n",
    "    dirs = os.listdir(os.path.join(dlc_dir, \"labeled-data\"))\n",
    "    dirs.sort()\n",
    "    dfs = []\n",
    "    \n",
    "    #-------------------------------------    \n",
    "    # Step 1: get the DLC labels in videos_picked and convert to LP format \n",
    "    #-------------------------------------    \n",
    "    print(\"Start generating <YOUR_LABELED_FRAMES>.csv!\")\n",
    "    # for curr_video in dirs[:num_videos]:\n",
    "    for curr_video in videos_picked:\n",
    "        print(\"----\", curr_video, \"----\",)\n",
    "        try:\n",
    "            # assume dlc format\n",
    "            header_rows = [0, 1, 2]\n",
    "            \n",
    "            # read the annotation file of current video \n",
    "            csv_file = glob(os.path.join(dlc_dir, \"labeled-data\", curr_video, \"CollectedData*.csv\"))[0]\n",
    "            print(f\"csv_file:{csv_file}\")\n",
    "            df_tmp = pd.read_csv(csv_file, header=header_rows, index_col=0)\n",
    "            \n",
    "            # convert the .DLC csv_file to LP format\n",
    "            if len(df_tmp.index.unique()) != df_tmp.shape[0]:\n",
    "                # print(\"new DLC labeling scheme that splits video/image in different cells!!\")\n",
    "                # new DLC labeling scheme that splits video/image in different cells\n",
    "                vids = df_tmp.loc[\n",
    "                       :, (\"Unnamed: 1_level_0\", \"Unnamed: 1_level_1\", \"Unnamed: 1_level_2\")]\n",
    "                imgs = df_tmp.loc[\n",
    "                       :, (\"Unnamed: 2_level_0\", \"Unnamed: 2_level_1\", \"Unnamed: 2_level_2\")]\n",
    "                # new_col = [f\"labeled-data/{v}/{i}\" for v, i in zip(vids, imgs)]\n",
    "                \n",
    "                # use the acutual video name\n",
    "                new_col = [f\"labeled-data/{curr_video}/{i}\" for i in imgs]\n",
    "\n",
    "                df_tmp1 = df_tmp.drop(\n",
    "                    (\"Unnamed: 1_level_0\", \"Unnamed: 1_level_1\", \"Unnamed: 1_level_2\"), axis=1,\n",
    "                )\n",
    "                df_tmp2 = df_tmp1.drop(\n",
    "                    (\"Unnamed: 2_level_0\", \"Unnamed: 2_level_1\", \"Unnamed: 2_level_2\"), axis=1,\n",
    "                )\n",
    "                df_tmp2.index = new_col\n",
    "                df_tmp = df_tmp2\n",
    "\n",
    "                # make the MultiIndex consistent through differet sessions (videos may be annotated by different scorer)\n",
    "                df_tmp = convert_header_to_dlc(df_tmp, model_name)\n",
    "                dfs.append(df_tmp)\n",
    "       \n",
    "        except IndexError:\n",
    "            print(f\"Could not find labels for {curr_video}; skipping\") \n",
    "            \n",
    "    #-----------------------------------------------------------------         \n",
    "    # Step 2: concatenate the annotation .csv files of videos_picked to\n",
    "    # <YOUR_LABELED_FRAMES>.csv\n",
    "    #------------------------------------- ----------------------------   \n",
    "    df_data = pd.concat(dfs)\n",
    "    # save concatenated labels\n",
    "    df_data.to_csv(save_lp_csv)\n",
    "    print(\"Finish generating <YOUR_LABELED_FRAMES>.csv!\")\n",
    "    print(\"-----\"*15)\n",
    "    print()\n",
    "    \n",
    "    #-----------------------------------------------------------------         \n",
    "    # Step 3: copying or generating mp4 video to lp_dir\n",
    "    # All unlabeled videos must be placed in a single directory. \n",
    "    # <VIDEO_DIR>/\n",
    "    #-----------------------------------------------------------------    \n",
    "    os.makedirs(lp_dir, exist_ok=True)\n",
    "    print(\"Start generating <VIDEO_DIR>/!\")\n",
    "    for curr_video in videos_picked:\n",
    "        video_file = glob(os.path.join(dlc_dir, \"videos\", f\"{curr_video}.*\"))[0]\n",
    "        print(f\"Working on: {video_file}\")\n",
    "\n",
    "        lp_video_dir = os.path.join(lp_dir, 'videos/')\n",
    "        Path(lp_video_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # convert avi. videos to mp4 format since LP only accepts mp4\n",
    "        if video_file.endswith(\".avi\"):\n",
    "            # save mp4 to lp_video_dir\n",
    "            inputfile  = video_file[:-4] + '.avi'\n",
    "            outputfile = video_file[:-4] + '.mp4'\n",
    "            outputfile = os.path.join(lp_video_dir, outputfile.split('/')[-1])\n",
    "\n",
    "            if not os.path.exists(outputfile):\n",
    "                print(\"Converting avi video files to be mp4 format!\")\n",
    "                print(\"outputfile:\", outputfile) \n",
    "                clip = moviepy.VideoFileClip(inputfile)\n",
    "                clip.write_videofile(outputfile)\n",
    "        else:\n",
    "            # Optional: copy video over\n",
    "            # if the behavior videos are very large, it may take long time\n",
    "            dst = os.path.join(lp_video_dir, video_file.split('/')[-1])     \n",
    "            if not os.path.exists(dst):\n",
    "                print(\"copying video files\")\n",
    "                shutil.copyfile(video_file, dst)\n",
    "    print(\"Finish generating <VIDEO_DIR>/!\")\n",
    "    print(\"-----\"*15)\n",
    "    print()\n",
    "    \n",
    "    #-----------------------------------------------------------------         \n",
    "    # Step 4: copying frames to lp_dir\n",
    "    # <LABELED_DATA_DIR>/\n",
    "    #----------------------------------------------------------------- \n",
    "    print(\"Start generating <LABELED_DATA_DIR>/!\")\n",
    "    for curr_video in videos_picked:  \n",
    "        # copy frames over\n",
    "        src = os.path.join(dlc_dir, \"labeled-data\", curr_video)\n",
    "        dst = os.path.join(lp_dir, \"labeled-data\", curr_video)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copytree(src, dst)\n",
    "    print(\"Finish generating <LABELED_DATA_DIR>/!\")\n",
    "    print(\"-----\"*15)\n",
    "    print()\n",
    "    \n",
    "    #-----------------------------------------------------------------     \n",
    "    # Step 5: check if the labeled frames in <YOUR_LABELED_FRAMES>.csv exist\n",
    "    # make sure the first column of <YOUR_LABELED_FRAMES>.csv matchs the path of labeled frames under lp_dir\n",
    "    #-----------------------------------------------------------------             \n",
    "    for im in df_data.index:\n",
    "        assert os.path.exists(os.path.join(lp_dir, im))\n",
    "#         assert os.path.exists(os.path.join(dlc_dir, im))\n",
    "\n",
    "    print(f\"The number of labeled frames: {len(df_data.index)}\")\n",
    "    \n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Converting the DLC project to Lightning Pose format\n",
    "\n",
    "**DeeplabCut assumes the following project directory structure:**\n",
    "```console\n",
    "    /path/to/DLC_project/\n",
    "      ├── labeled-data/\n",
    "      └── videos/\n",
    "```\n",
    "* `labeled-data`: This directory stores the frames used to create the DLC training dataset. Frames from different videos are stored in separate subdirectories. Each frame has a filename related to the temporal index within the corresponding video, which allows the user to trace every frame back to its origin.\n",
    "\n",
    "* `videos`: Directory of video links or videos. \n",
    "\n",
    "An example of DLC project is `/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lightning Pose assumes the following [project directory structure](https://lightning-pose.readthedocs.io/en/latest/source/user_guide/directory_structure.html)**, as in the example dataset\n",
    "provided in [mirror-mouse](https://github.com/danbider/lightning-pose/tree/main/data/mirror-mouse-example).\n",
    "```console\n",
    "    /path/to/LP_project/\n",
    "      ├── <LABELED_DATA_DIR>/\n",
    "      ├── <VIDEO_DIR>/\n",
    "      └── <YOUR_LABELED_FRAMES>.csv\n",
    "```\n",
    "* `<YOUR_LABELED_FRAMES>.csv`: a table with keypoint labels (rows: frames; columns: keypoints). \n",
    "Note that this file can take any name, and needs to be specified in the config file under \n",
    "`data.csv_file`.\n",
    "\n",
    "* `<LABELED_DATA_DIR>/`: contains images that correspond to the labels, and can include subdirectories.\n",
    "The directory name, any subdirectory names, and image names are all flexible, as long as they are\n",
    "consistent with the first column of `<YOUR_LABELED_FRAMES>.csv`.\n",
    "\n",
    "* `<VIDEO_DIR>/`: when training semi-supervised models, the videos in this directory will be used \n",
    "for computing the unsupervised losses. This directory can take any name, and needs to be specified \n",
    "in the config file under `data.video_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert a DLC project to LP format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# set up the path to DLC project, LP training data and LP outputs \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# question for Di: does this scorer name need to match the scorer name in the CSV?\n",
    "#  - No, it does not\n",
    "\n",
    "# use Han's data\n",
    "# set up the path to the DLC project\n",
    "scorer_name  = \"Han_behavior_data_test\"  \n",
    "# scorer_name  = \"Han_behavior_data\"   \n",
    "project_name = \"Foraging_Bot-Han_Lucas-2022-04-27\" \n",
    "DLC_data_dir = os.path.join(\"/root/capsule/data/s3_video/DLC_projects\", project_name)\n",
    "# if columns_to_pick is null, using all the keypoints for training \n",
    "columns_to_pick = []\n",
    "# assume dlc format\n",
    "header_rows = [0, 1, 2]\n",
    "\n",
    "\n",
    "# # use VBN data\n",
    "# scorer_name  = \"VBN_behavior_DLC\"\n",
    "# scorer_name  = \"VBN_behavior_DLC_test\"\n",
    "# project_name = \"face\"\n",
    "# DLC_data_dir = os.path.join(\"/root/capsule/data/vbn_dlc_all_4\", project_name)\n",
    "# columns_to_pick = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 24, 27, 28, 39]\n",
    "# header_rows = [0, 1, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_dir: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/\n",
      "['bottom_face_1-0000', 'bottom_face_101-0000', 'bottom_face_110-0000', 'bottom_face_114-0000', 'bottom_face_115-0000', 'bottom_face_116-0000', 'bottom_face_137-0000', 'bottom_face_164-0000', 'bottom_face_171-0000', 'bottom_face_186-0000', 'bottom_face_197-0000', 'bottom_face_20-0000', 'bottom_face_205-0000', 'bottom_face_37-0000', 'bottom_face_41-0000', 'bottom_face_484-0000', 'bottom_face_49-0000', 'bottom_face_5-0000', 'bottom_face_521-0000', 'bottom_face_533-0000', 'bottom_face_593-0000', 'bottom_face_6-0000', 'bottom_face_65-0000', 'bottom_face_670-0000', 'bottom_face_718-0000', 'bottom_face_857-0000', 'bottom_face_861-0000', 'bottom_face_866-0000', 'bottom_face_888-0000'] 29\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# get the videos which contain labeled frames\n",
    "# ---------------------------------------\n",
    "DLC_video_dir   = os.path.join(DLC_data_dir, 'videos/')\n",
    "\n",
    "DLC_video_files = get_videos_in_dir(DLC_video_dir)\n",
    "video_names = []\n",
    "for video_file in DLC_video_files:\n",
    "    video_names.append(video_file.split('/')[-1][:-4])\n",
    "print(video_names, len(video_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# select videos you want to train on\n",
    "# ---------------------------------------\n",
    "\n",
    "# for Han's data\n",
    "videos_picked = ['bottom_face_1-0000',\n",
    "                 'bottom_face_5-0000',\n",
    "                 'bottom_face_20-0000',\n",
    "                 'bottom_face_49-0000',\n",
    "                 'bottom_face_116-0000',\n",
    "                 'bottom_face_484-0000',\n",
    "                 'bottom_face_533-0000',\n",
    "                 'bottom_face_670-0000']\n",
    "\n",
    "videos_picked = ['bottom_face_1-0000', 'bottom_face_101-0000', \n",
    "                 'bottom_face_110-0000', 'bottom_face_114-0000', \n",
    "                 'bottom_face_115-0000', 'bottom_face_116-0000', \n",
    "                 'bottom_face_137-0000', 'bottom_face_164-0000', \n",
    "                 'bottom_face_171-0000', 'bottom_face_186-0000']\n",
    "\n",
    "# # for VBN data\n",
    "# videos_picked = ['1128520325_585326_20210915.face', \n",
    "#                  '1122903357_570302_20210818.face', \n",
    "#                  '1052533639_530862_20200924.face', \n",
    "#                  ]\n",
    "\n",
    "# if videos_picked is null, using all labels instead of only using the labels in selected videos\n",
    "if len(videos_picked) == 0:\n",
    "    videos_picked = video_names\n",
    "num_videos = len(videos_picked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# set up the path to the LP data and outputs\n",
    "# ---------------------------------------\n",
    "LP_data_dir = os.path.join(\"/root/capsule/scratch/DLC_dataset_for_LP\",\n",
    "                            scorer_name,\n",
    "                            project_name)\n",
    "Path(LP_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LP_output_dir = os.path.join(LP_data_dir, \"outputs\")\n",
    "Path(LP_output_dir).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27\n",
      "total 12\n",
      "drwxr-xr-x 3 root root 6144 Mar  4 22:57 .\n",
      "drwxr-xr-x 3 root root 6144 Mar  4 22:57 ..\n",
      "drwxr-xr-x 2 root root 6144 Mar  4 22:57 outputs\n"
     ]
    }
   ],
   "source": [
    "print(LP_data_dir)\n",
    "! ls -la $LP_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting a DLC project to LP format .....\n",
      "\n",
      "Converting DLC project located at /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27 to LP project located at /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27\n",
      "Start generating <YOUR_LABELED_FRAMES>.csv!\n",
      "---- bottom_face_1-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_1-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_101-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_101-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_110-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_110-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_114-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_114-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_115-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_115-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_116-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_116-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_137-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_137-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_164-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_164-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_171-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_171-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "---- bottom_face_186-0000 ----\n",
      "csv_file:/root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_186-0000/CollectedData_Han_Lucas.csv\n",
      "The number of keypoint: 17\n",
      "Finish generating <YOUR_LABELED_FRAMES>.csv!\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Start generating <VIDEO_DIR>/!\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_1-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_1-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_1-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_1-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_1-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_101-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_101-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_101-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_101-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_101-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_110-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_110-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_110-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_110-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_110-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_114-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_114-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_114-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_114-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_114-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_115-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_115-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_115-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_115-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_115-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_116-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_116-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_116-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_116-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_116-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_137-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_137-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_137-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_137-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_137-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_164-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_164-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_164-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_164-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_164-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_171-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_171-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_171-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_171-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_171-0000.mp4\n",
      "Working on: /root/capsule/data/s3_video/DLC_projects/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_186-0000.avi\n",
      "Converting avi video files to be mp4 format!\n",
      "outputfile: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_186-0000.mp4\n",
      "Moviepy - Building video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_186-0000.mp4.\n",
      "Moviepy - Writing video /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_186-0000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/bottom_face_186-0000.mp4\n",
      "Finish generating <VIDEO_DIR>/!\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Start generating <LABELED_DATA_DIR>/!\n",
      "Finish generating <LABELED_DATA_DIR>/!\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "The number of labeled frames: 156\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Convert DLC project to LP format\n",
    "# ---------------------------------------\n",
    "print(f\"Converting a DLC project to LP format .....\")\n",
    "\n",
    "# Call dlc2lp() to generate the LP dataset with the following directory struture\n",
    "#     /path/to/LP_project/\n",
    "#       ├── <LABELED_DATA_DIR>/\n",
    "#       ├── <VIDEO_DIR>/\n",
    "#       └── <YOUR_LABELED_FRAMES>.csv\n",
    "\n",
    "# set up the path to <YOUR_LABELED_FRAMES>.csv\n",
    "LP_labels_file_all = os.path.join(LP_data_dir, f\"CollectedData.csv\")\n",
    "\n",
    "model_name = f\"trained_with_{num_videos}videos\" # the name for LP model\n",
    "df_data = dlc2lp(DLC_data_dir, \n",
    "               LP_data_dir, \n",
    "               model_name, \n",
    "               LP_labels_file_all,\n",
    "               videos_picked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27\n",
      "total 112\n",
      "drwxr-xr-x  5 root root  6144 Mar  4 22:58 .\n",
      "drwxr-xr-x  3 root root  6144 Mar  4 22:57 ..\n",
      "-rw-r--r--  1 root root 91886 Mar  4 22:57 CollectedData.csv\n",
      "drwxr-xr-x 12 root root  6144 Mar  4 22:58 labeled-data\n",
      "drwxr-xr-x  2 root root  6144 Mar  4 22:57 outputs\n",
      "drwxr-xr-x  2 root root  6144 Mar  4 22:58 videos\n"
     ]
    }
   ],
   "source": [
    "print(LP_data_dir)\n",
    "! ls -la $LP_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint names: ['tongueTip', 'tongueLeftFront', 'tongueRightFront', 'tongueLeftBack', 'tongueRightBack', 'LickportLeft', 'LickportRight', 'nosetip', 'jaw', 'pawL', 'pawR', 'WLup', 'WLmid', 'WLbot', 'WRup', 'WRmid', 'WRbot'], 17\n",
      "\n",
      "selected keypoints: ['tongueTip', 'tongueLeftFront', 'tongueRightFront', 'tongueLeftBack', 'tongueRightBack', 'LickportLeft', 'LickportRight', 'nosetip', 'jaw', 'pawL', 'pawR', 'WLup', 'WLmid', 'WLbot', 'WRup', 'WRmid', 'WRbot'], 17\n",
      "\n",
      "save masked LP label file to /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/CollectedData.masked.csv\n",
      "\n",
      "The LP label file for training: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/CollectedData.masked.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# select keypoints to train\n",
    "# we could include all keypoints or certain keypoints for training \n",
    "# LP_labels_file_all: <.csv> inludes all keypoints\n",
    "# LP_labels_file_masked: <.masked.csv> only contains the columns of selected keypoints\n",
    "# ---------------------------------------\n",
    "\n",
    "# get the keypoints names\n",
    "keypoint_names = get_keypoint_names(LP_labels_file_all, header_rows)\n",
    "print(f\"keypoint names: {keypoint_names}, {len(keypoint_names)}\")\n",
    "\n",
    "# select keypoints for training\n",
    "if len(columns_to_pick) == 0:\n",
    "    keypoint_to_pick = keypoint_names # if columns_to_pick is null, using all the keypoints for training \n",
    "else:\n",
    "    keypoint_to_pick = [ keypoint_names[col] for col in columns_to_pick ]\n",
    "print(f\"\\nselected keypoints: {keypoint_to_pick}, {len(keypoint_to_pick)}\")\n",
    "\n",
    "# extract the columns of selected keypoints\n",
    "df_data_masked = mask_df(LP_labels_file_all, header_rows, keypoint_to_pick)\n",
    "\n",
    "# save masked annotation files which only contains the labels of the selected keypoints\n",
    "LP_labels_file_masked = LP_labels_file_all.replace(\".csv\", \".masked.csv\")\n",
    "df_data_masked.to_csv(LP_labels_file_masked)\n",
    "print(f\"\\nsave masked LP label file to {LP_labels_file_masked}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# set up the LP annotation file\n",
    "# ---------------------------------------\n",
    "LP_labels_file = LP_labels_file_masked \n",
    "# LP_labels_file = LP_labels_file_all\n",
    "print(f\"\\nThe LP label file for training: {LP_labels_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/outputs\n",
      "total 0\n",
      "\n",
      "Store training and testing results to /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/outputs\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# change the working dir\n",
    "# ---------------------------------------\n",
    "# LP_output_dir = os.getcwd()\n",
    "%pwd\n",
    "%cd $LP_output_dir\n",
    "%pwd\n",
    "! ls -lt $LP_output_dir\n",
    "print(f\"\\nStore training and testing results to {LP_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: update the yaml config file\n",
    "\n",
    "After generating LP dataset, you will need to update your config file with the correct paths. This file points to data directories, defines the type of models to fit, and specifies a wide range of hyperparameters. The default configuration file at [here](https://github.com/danbider/lightning-pose/blob/main/scripts/configs/config_default.yaml) enumerates all possible hyperparameters needed for building and training a model. See [here](https://lightning-pose.readthedocs.io/en/latest/source/user_guide/config_file.html) for more information.\n",
    "\n",
    "**To create the yaml config file, follow these steps:**\n",
    "* [(1) update the path to the training data](#(1)-update-the-path-to-the-training-data)\n",
    "* [(2) update the testing video path](#(2)-update-the-testing-video-path)\n",
    "* [(3) update the image dimensions ](#(3)-update-the-image-dimensions)\n",
    "* [(4) update the keypoint info](#(4)-update-the-keypoint-info)\n",
    "* [(5) set up training parameters](#(5)-set-up-training-parameters)\n",
    "* [(6) set up unsupervised losses](#(6)-set-up-unsupervised-losses)\n",
    "<!-- * [(7) set up the fully-supervised training](#(7)-set-up-fully-supervised-training) -->\n",
    "* [(7) save the updated LP config file](#(7)-save-the-updated-LP-config-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    " Below is a list of some commonly modified arguments in a LP config file related to model architecture/training. When training a model on a new dataset, you should copy/paste the default config and update the\n",
    "arguments to match your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - data.csv_file: location of labels\n",
    "- data.video_dir: location of unlabeled videos\n",
    "- data.num_keypoints: total number of keypoints\n",
    "- data.columns_for_singleview_pca: list of indices of keypoints used for pca singleview loss\n",
    "<br/><br/>\n",
    "\n",
    "- training.train_batch_size (default: `16`) - batch size for labeled data\n",
    "- training.train_prob (default: `0.8`) - fraction of labeled data used for training\n",
    "- training.val_prob (default: `0.1`) - fraction of labeled data used for validation (remaining used for test)\n",
    "- training.min_epochs (default: `300`)\n",
    "- training.max_epochs (default: `750`)\n",
    "<br/><br/>\n",
    "\n",
    "- model.model_type (default: `heatmap`)\n",
    "  - regression: model directly outputs an (x, y) prediction for each keypoint; not recommended\n",
    "  - heatmap: model outputs a 2D heatmap for each keypoint\n",
    "  - heatmap_mhcrnn: the \"multi-head convolutional RNN\", this model takes a temporal window of\n",
    "    frames as input, and outputs two heatmaps: one \"context-aware\" and one \"static\". The prediction\n",
    "    with the highest confidence is automatically chosen. Must also set `model.do_context=True`.\n",
    "- model.losses_to_use (default: `[]`) - this argument relates to the unsupervised losses. An empty\n",
    "  list indicates a fully supervised model. Each element of the list corresponds to an unsupervised\n",
    "  loss. For example,\n",
    "  `model.losses_to_use=[pca_multiview,temporal]` will fit both a pca_multiview loss and a temporal\n",
    "  loss. Options include:\n",
    "  - pca_multiview: penalize inconsistencies between multiple camera views\n",
    "  - pca_singleview: penalize implausible body configurations\n",
    "  - temporal: penalize large temporal jumps\n",
    "<br/><br/>\n",
    "\n",
    "- eval.test_videos_directory - str with an absolute path to a directory containing videos for prediction.\n",
    "- eval.confidence_thresh_for_vid (default: `0.9`) - confidence threshold for plotting a vid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LP default config file, and update parameter wrt your own behavior data\n",
    "LP_config_template = \"/lightning-pose/scripts/configs/config_default.yaml\"\n",
    "with open(LP_config_template, 'r') as file:\n",
    "    param_updated = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) update the path to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute path to a directory containing LP labeled frames. \n",
    "# Frames from different videos are stored in separate subdirectories.\n",
    "param_updated['data'][\"data_dir\"] = LP_data_dir \n",
    "\n",
    "# absolute path to the LP annotation file. \n",
    "# Each frame has a filename related to the temporal index within the corresponding video, \n",
    "# which allows the user to trace every frame back to its origin.\n",
    "param_updated['data'][\"csv_file\"] = LP_labels_file\n",
    "\n",
    "# absolute path to a directory containing videos for training\n",
    "LP_video_dir = os.path.join(LP_data_dir, 'videos/')\n",
    "param_updated['data'][\"video_dir\"] = LP_video_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) update the testing video path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute path to a directory containing videos for prediction.\n",
    "param_updated['eval']['test_videos_directory'] = LP_video_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) update the image dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path to the first labeled frame:/root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/labeled-data/bottom_face_1-0000/img0026.png\n"
     ]
    }
   ],
   "source": [
    "# load ground truth\n",
    "labels_df = pd.read_csv(LP_labels_file)\n",
    "\n",
    "# get the absolute path to the first labeled frame \n",
    "frame_1st = os.path.join( LP_data_dir, labels_df['scorer'].to_list()[2])\n",
    "print(f\"The path to the first labeled frame:{frame_1st}\")\n",
    "\n",
    "# get the image dimension\n",
    "image = Image.open(frame_1st).convert(\"RGB\")\n",
    "# set up resize dimension, LP requires its a multiple of 128 to accelerate training.\n",
    "# Optional: limit image size to 640 to avoid OOM\n",
    "param_updated['data'][\"image_resize_dims\"][\"width\"]  = closest(image.size[0]) if image.size[0] < 640 else 640\n",
    "param_updated['data'][\"image_resize_dims\"][\"height\"] = closest(image.size[1]) if image.size[1] < 640 else 640\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) update the keypoint info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint names: ['tongueTip', 'tongueLeftFront', 'tongueRightFront', 'tongueLeftBack', 'tongueRightBack', 'LickportLeft', 'LickportRight', 'nosetip', 'jaw', 'pawL', 'pawR', 'WLup', 'WLmid', 'WLbot', 'WRup', 'WRmid', 'WRbot']\n",
      "The number of keypoints: 17\n"
     ]
    }
   ],
   "source": [
    "# get the keypoints names\n",
    "keypoint_names = get_keypoint_names(LP_labels_file, header_rows)\n",
    "num_bodyparts  = len(keypoint_names)\n",
    "print(f\"keypoint names: {keypoint_names}\")\n",
    "print(f\"The number of keypoints: {num_bodyparts}\")\n",
    "\n",
    "param_updated['data'][\"num_keypoints\"] = num_bodyparts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) set up training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training frames include both visible and occluded keypoint, Lightning pose will output the confidence value always close to 1 for occluded keypoints (i.e, tongue). \n",
    "To address this issue, LP has a non-default option that includes missing data in the loss by comparing the predicted heatmap to a uniform heatmap. \n",
    "To set the non-default option, add the following option to your config yaml file (under the \"training\" key):\n",
    "```\n",
    "training:\n",
    "    uniform_heatmaps_for_nan_keypoints: true\n",
    "```\n",
    "See [here](https://lightning-pose.readthedocs.io/en/latest/source/faqs.html#faq-nan-heatmaps) for more information about why the network produce high confidence values for keypoints even when they are occluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_updated[\"training\"][\"uniform_heatmaps_for_nan_keypoints\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) set up unsupervised losses \n",
    "For a detailed mathematical description of the losses, see the [Lightning Pose paper](https://www.biorxiv.org/content/10.1101/2023.04.28.538703v1). \n",
    "\n",
    "See [here](https://lightning-pose.readthedocs.io/en/latest/source/user_guide_advanced/unsupervised_losses.html) for more details on how to use unsupervised losses (i.e., Temporal continuity, Pose plausibility and Multiview consistency) and set up hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to apply unsupervised losses on unlabeled video data, model.losses_to_use must be non-empty \n",
    "# # (which indicates a fully supervised model). \n",
    "# param_updated['model'][\"losses_to_use\"] = [\"pca_singleview\"] # or multiple losses: [temporal,pca_singleview]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns_for_singleview_pca: list of indices of keypoints used for pca singleview loss.\n",
    "\n",
    "Ensure the number of samples is greater than the obervation dimensions (have more rows than columns after doing nan filtering)!\n",
    "Since each keypoint is 2-dimensional (x, y coords), if there are K keypoints labeled on each frame then each pose \n",
    "is described by a 2K-dimensional vector. Therefore, at least 2K frames need to be labeled to compute the PCA subspace.\n",
    "\n",
    "If the error massage \"cannot fit PCA with N samples < M observation dimensions\" occures, \n",
    "reselect or reduce the columns_for_singleview_pca or enlarge the training data size.\n",
    "\n",
    "It is up to the user to select which keypoints are included in the Pose plausibility loss. \n",
    "Including static keypoints (e.g. those marking a corner of an arena) are generally not helpful. \n",
    "Also be careful to not include keypoints that are often occluded, like the tongue. If these keypoints \n",
    "are included the loss will try to localize them even when they are occluded, which might be unhelpful if you \n",
    "want to use the confidence of the outputs as a lick detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.tongueTip\n",
      "1.tongueLeftFront\n",
      "2.tongueRightFront\n",
      "3.tongueLeftBack\n",
      "4.tongueRightBack\n",
      "5.LickportLeft\n",
      "6.LickportRight\n",
      "7.nosetip\n",
      "8.jaw\n",
      "9.pawL\n",
      "10.pawR\n",
      "11.WLup\n",
      "12.WLmid\n",
      "13.WLbot\n",
      "14.WRup\n",
      "15.WRmid\n",
      "16.WRbot\n"
     ]
    }
   ],
   "source": [
    "for ind, name in enumerate(keypoint_names):\n",
    "    print(f\"{ind}.{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_updated['data'][\"columns_for_singleview_pca\"] = [ i for i in range(num_bodyparts) ] \n",
    "# The numbers used should correspond to the order of the keypoints in the labeled csv file. \n",
    "param_updated['data'][\"columns_for_singleview_pca\"] = [ 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) save the updated LP config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute path to save updated LP config file\n",
    "LP_config_file = os.path.join(LP_data_dir,  \n",
    "                              f'{project_name}.config.yaml') \n",
    "\n",
    "# save  \n",
    "with open(LP_config_file, 'w') as yaml_file:\n",
    "    yaml.dump(param_updated, yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/Foraging_Bot-Han_Lucas-2022-04-27.config.yaml'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LP_config_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Check if the training data exist\n",
    "\n",
    "```console\n",
    "    /path/to/LP_project/\n",
    "      ├── <LABELED_DATA_DIR>/\n",
    "      ├── <VIDEO_DIR>/\n",
    "      └── <YOUR_LABELED_FRAMES>.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP training data: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27\n",
      "LP training videos: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/videos/\n",
      "LP annotation file: /root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/CollectedData.masked.csv\n"
     ]
    }
   ],
   "source": [
    "# load config file\n",
    "cfg = OmegaConf.load(LP_config_file)\n",
    "\n",
    "# print(\"Our Hydra config file:\")\n",
    "# pretty_print_cfg(cfg)\n",
    "\n",
    "# path handling for the dataset\n",
    "data_dir, video_dir = return_absolute_data_paths(data_cfg=cfg.data)\n",
    "\n",
    "# <LABELED_DATA_DIR>: cfg.data.data_dir, the absolute path to the labeled frames\n",
    "assert os.path.isdir(cfg.data.data_dir), \"data_dir not a valid directory\"\n",
    "\n",
    "# <VIDEO_DIR>: cfg.data.video_dir, the absolute path to the videos\n",
    "assert os.path.isdir(cfg.data.video_dir), \"video_dir not a valid directory\"\n",
    "\n",
    "# <YOUR_LABELED_FRAMES>.csv: cfg.data.csv_file, the absolute path to the annotation file\n",
    "df_tmp = pd.read_csv(cfg.data.csv_file, header=header_rows, index_col=0)\n",
    "for img in df_tmp.index:\n",
    "    assert os.path.exists(os.path.join(cfg.data.data_dir, img))\n",
    "    \n",
    "print(f\"LP training data: {data_dir}\")\n",
    "print(f\"LP training videos: {video_dir}\")\n",
    "print(f\"LP annotation file: {cfg.data.csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D47Ko2u-R5Ko"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/outputs\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xw6_WU3dVC30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dlc image augmentation pipeline\n",
      "\n",
      " Initializing a HeatmapTracker instance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.openmmlab.com/mmpose/animal/resnet/res50_ap10k_256x256-35760eb8_20211029.pth\" to /root/.cache/torch/hub/checkpoints/res50_ap10k_256x256-35760eb8_20211029.pth\n",
      "100%|██████████| 130M/130M [00:30<00:00, 4.48MB/s] \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# build dataset, model, and trainer\n",
    "\n",
    "# make training short for a demo (we usually do 300)\n",
    "# (approx 2 mins for training Han's data using fully-supervised learning with epoch=55)\n",
    "# (approx 6 mins for training Han's data using semi-supervised learning (losses_to_use: ['pca_singleview']) with epoch=55)\n",
    "cfg.training.min_epochs = 200\n",
    "cfg.training.max_epochs = 400\n",
    "cfg.training.batch_size = 8\n",
    "\n",
    "# build imgaug transform\n",
    "imgaug_transform = get_imgaug_transform(cfg=cfg)\n",
    "\n",
    "# build dataset\n",
    "dataset = get_dataset(cfg=cfg, data_dir=data_dir, imgaug_transform=imgaug_transform)\n",
    "\n",
    "# build datamodule; breaks up dataset into train/val/test\n",
    "data_module = get_data_module(cfg=cfg, dataset=dataset, video_dir=video_dir)\n",
    "\n",
    "# build loss factory which orchestrates different losses\n",
    "loss_factories = get_loss_factories(cfg=cfg, data_module=data_module)\n",
    "\n",
    "# build model\n",
    "model = get_model(cfg=cfg, data_module=data_module, loss_factories=loss_factories)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Set up and run training\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# logger\n",
    "logger = pl.loggers.TensorBoardLogger(\"tb_logs\", name=cfg.model.model_name)\n",
    "\n",
    "# early stopping, learning rate monitoring, model checkpointing, backbone unfreezing\n",
    "callbacks = get_callbacks(cfg)\n",
    "\n",
    "# calculate number of batches for both labeled and unlabeled data per epoch\n",
    "limit_train_batches = calculate_train_batches(cfg, dataset)\n",
    "\n",
    "# set up trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=cfg.training.max_epochs,\n",
    "    min_epochs=cfg.training.min_epochs,\n",
    "    check_val_every_n_epoch=cfg.training.check_val_every_n_epoch,\n",
    "    log_every_n_steps=cfg.training.log_every_n_steps,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    limit_train_batches=limit_train_batches,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: tb_logs/test\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labeled images in the full dataset (train+val+test): 156\n",
      "Size of -- train set: 124, val set: 15, test set: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone          | Sequential         | 23.5 M\n",
      "1 | loss_factory      | LossFactory        | 0     \n",
      "2 | upsampling_layers | Sequential         | 81.0 K\n",
      "3 | rmse_loss         | RegressionRMSELoss | 0     \n",
      "---------------------------------------------------------\n",
      "134 K     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.6 M    Total params\n",
      "94.356    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ab5ece53df4c57b47ee1607553612e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but the required `min_epochs=200` or `min_steps=None` has not been met. Training will continue...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training duration: 0:08:05.028949\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "# train model!\n",
    "# Train the model \n",
    "# (approx 2 mins for training Han's data using fully-supervised learning with epoch=55)\n",
    "# (approx 6 mins for training Han's data using semi-supervised learning (losses_to_use: ['pca_singleview']) with epoch=55)\n",
    "trainer.fit(model=model, datamodule=data_module)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nTraining duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/outputs'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LP_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 276916\n",
      "-rw-r--r-- 1 root root 283558168 Mar  4 18:52 'epoch=184-step=1480.ckpt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Once training has completed, use the checkpoint that corresponds to the best performance you found during the training process.\n",
    "# a checkpoint: a version of the model. \n",
    "# check the trained model\n",
    "! ls -lt \"/root/capsule/scratch/DLC_dataset_for_LP/Han_behavior_data_test/Foraging_Bot-Han_Lucas-2022-04-27/outputs/tb_logs/test/version_0/checkpoints/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
